# Kaggle-Natural-Language-Processing
Kaggle-Natural-Language-Processing



-------
-------

# Natural Language Processing
https://www.youtube.com/playlist?list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu

Yannic Kilcher

### 1 27:33  GPT-2: Language Models are Unsupervised Multitask Learners
Paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

### 2 40:13  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Paper: https://arxiv.org/abs/1810.04805

### 3 18:19  Stochastic RNNs without Teacher-Forcing
Paper: https://arxiv.org/abs/1806.04550

### 4 27:07  Attention Is All You Need
Paper: https://arxiv.org/abs/1706.03762

### 5 30:06  XLNet: Generalized Autoregressive Pretraining for Language Understanding
Paper: https://arxiv.org/abs/1906.08237

### 6 19:15  RoBERTa: A Robustly Optimized BERT Pretraining Approach
Paper: https://arxiv.org/abs/1907.11692

### 7 30:22  LeDeepChef üë®‚Äçüç≥ Deep Reinforcement Learning Agent for Families of Text-Based Games
Paper: https://arxiv.org/abs/1909.01646

### 8 29:12  Reformer: The Efficient Transformer
Paper: https://arxiv.org/abs/2001.04451

### 9 21:18  Turing-NLG, DeepSpeed and the ZeRO optimizer


### 10 24:16  Deep Learning for Symbolic Mathematics
Paper: https://arxiv.org/abs/1912.01412

### 11 18:59  Evaluating NLP Models via Contrast Sets
Paper: https://arxiv.org/abs/2004.02709

### 12 18:15  Imputer: Sequence Modelling via Imputation and Dynamic Programming
Paper: https://arxiv.org/abs/2002.08926

### 13 26:36  Longformer: The Long-Document Transformer
Paper: https://arxiv.org/abs/2004.05150

### 14 11:21  I talk to the new Facebook Blender Chatbot
Paper: https://arxiv.org/abs/2004.13637

### 15 31:48  TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)
Paper: https://arxiv.org/abs/2004.02349

### 16 1:02:41  [Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)


### 17 11:11  [News] OpenAI Model Generates Python Code


### 18 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)


### 19 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)
Paper: https://arxiv.org/abs/2005.00561

### 20 1:04:30  GPT-3: Language Models are Few-Shot Learners (Paper Explained)
Paper: https://arxiv.org/abs/2005.14165

### 21 48:21  Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)
Paper: https://arxiv.org/abs/2005.00743

### 22 30:11  Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)
Paper: https://arxiv.org/abs/2005.07683

### 23 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)


### 24 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)
Paper: https://arxiv.org/abs/2004.04696

### 25 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)


### 26 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)
Paper: https://arxiv.org/abs/2006.03511

### 27 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 28 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 29 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 30 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 31 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 32 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 33 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)
Paper: https://arxiv.org/abs/2006.04768

### 34 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)


### 35 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)
Paper: https://arxiv.org/abs/2006.06666

### 36 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)


### 37 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)
Paper: https://arxiv.org/abs/2006.06462

### 38 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)


### 39 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)
Paper: https://arxiv.org/abs/2006.16668

### 40 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)


### 41 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)
Paper: https://arxiv.org/abs/2006.15222

### 42 48:06  Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)
Paper: https://arxiv.org/abs/2006.16236

### 43 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality


### 44 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality
Paper: https://arxiv.org/abs/1310.4546

### 45 34:30  Big Bird: Transformers for Longer Sequences (Paper Explained)
Paper: https://arxiv.org/abs/2007.14062

### 46 1:05:16  Hopfield Networks is All You Need (Paper Explained)
Paper: https://arxiv.org/abs/2008.02217

### 47 1:00:41  REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)
Paper: https://arxiv.org/abs/2002.08909

### 48 45:30  Learning to summarize from human feedback (Paper Explained)
Paper: https://arxiv.org/abs/2009.01325

### 49 54:39  Rethinking Attention with Performers (Paper Explained)
Paper: https://arxiv.org/abs/2009.14794

### 50 52:16  Language Models are Open Knowledge Graphs (Paper Explained)
Paper: https://arxiv.org/abs/2010.11967

### 51 1:03:18  Extracting Training Data from Large Language Models (Paper Explained)
Paper: https://arxiv.org/abs/2012.07805

### 52 55:46  OpenAI DALL¬∑E: Creating Images from Text (Blog Post Explained)
Blog: https://openai.com/blog/dall-e/

### 53 48:07  OpenAI CLIP: ConnectingText and Images (Paper Explained)
Paper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf

### 54 33:47  Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Paper: https://arxiv.org/abs/2101.03961

### 55 43:51  Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)
Paper: https://arxiv.org/abs/2002.09402

### 56 48:12  Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)
Paper: https://arxiv.org/abs/2102.03902

### 57 45:14  DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)
Paper: https://arxiv.org/abs/2006.03654

### 58 31:22  ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation
Paper: https://ofir.io/train_short_test_long.pdf

### 59 36:37  ‚àû-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)
Paper: https://arxiv.org/abs/2109.00301

### 60 13:19  Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset
Paper: https://arxiv.org/abs/2109.07958

### 61 45:22  Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)
Paper: https://arxiv.org/abs/2110.07178

### 62 57:07  Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained
Paper: https://arxiv.org/abs/2111.12763

-------
-------

