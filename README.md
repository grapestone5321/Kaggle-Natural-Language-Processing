# Kaggle-Natural-Language-Processing
Kaggle-Natural-Language-Processing

-------
-------

# "Getting Started" competition

## Natural Language Processing with Disaster Tweets
Predict which Tweets are about real disasters and which ones are not

https://www.kaggle.com/c/nlp-getting-started

-------

## Task

In this competition, you‚Äôre challenged to build a machine learning model that predicts which Tweets are about real disasters and which one‚Äôs aren‚Äôt. You‚Äôll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.

-------

## Evaluation

Submissions are evaluated using F1 between the predicted and expected answers.


-------

## NLP Getting Started Tutorial
https://www.kaggle.com/philculliton/nlp-getting-started-tutorial?scriptVersionId=25459258

NLP - or Natural Language Processing - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.

In this tutorial we'll look at this competition's dataset, use a simple technique to process it, build a machine learning model, and submit predictions for a score!

-------
-------

# Natural Language Processing
https://www.youtube.com/playlist?list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu

Yannic Kilcher

### 1 27:33  GPT-2: Language Models are Unsupervised Multitask Learners
Paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

### 2 40:13  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Paper: https://arxiv.org/abs/1810.04805

24 May 2019

### 3 18:19  Stochastic RNNs without Teacher-Forcing
Paper: https://arxiv.org/abs/1806.04550

### 4 27:07  Attention Is All You Need
Paper: https://arxiv.org/abs/1706.03762

### 5 30:06  XLNet: Generalized Autoregressive Pretraining for Language Understanding
Paper: https://arxiv.org/abs/1906.08237

### 6 19:15  RoBERTa: A Robustly Optimized BERT Pretraining Approach
Paper: https://arxiv.org/abs/1907.11692

### 7 30:22  LeDeepChef üë®‚Äçüç≥ Deep Reinforcement Learning Agent for Families of Text-Based Games
Paper: https://arxiv.org/abs/1909.01646

### 8 29:12  Reformer: The Efficient Transformer
Paper: https://arxiv.org/abs/2001.04451

### 9 21:18  Turing-NLG, DeepSpeed and the ZeRO optimizer


### 10 24:16  Deep Learning for Symbolic Mathematics
Paper: https://arxiv.org/abs/1912.01412

### 11 18:59  Evaluating NLP Models via Contrast Sets
Paper: https://arxiv.org/abs/2004.02709

### 12 18:15  Imputer: Sequence Modelling via Imputation and Dynamic Programming
Paper: https://arxiv.org/abs/2002.08926

### 13 26:36  Longformer: The Long-Document Transformer
Paper: https://arxiv.org/abs/2004.05150

2 Dec 2020

### 14 11:21  I talk to the new Facebook Blender Chatbot
Paper: https://arxiv.org/abs/2004.13637

### 15 31:48  TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)
Paper: https://arxiv.org/abs/2004.02349

### 16 1:02:41  [Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)


### 17 11:11  [News] OpenAI Model Generates Python Code


### 18 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)


### 19 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)
Paper: https://arxiv.org/abs/2005.00561

### 20 1:04:30  GPT-3: Language Models are Few-Shot Learners (Paper Explained)
Paper: https://arxiv.org/abs/2005.14165

### 21 48:21  Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)
Paper: https://arxiv.org/abs/2005.00743

### 22 30:11  Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)
Paper: https://arxiv.org/abs/2005.07683

### 23 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)


### 24 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)
Paper: https://arxiv.org/abs/2004.04696

### 25 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)


### 26 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)
Paper: https://arxiv.org/abs/2006.03511

### 27 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 28 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 29 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 30 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 31 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 32 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 33 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)
Paper: https://arxiv.org/abs/2006.04768

### 34 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)


### 35 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)
Paper: https://arxiv.org/abs/2006.06666

### 36 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)


### 37 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)
Paper: https://arxiv.org/abs/2006.06462

### 38 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)


### 39 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)
Paper: https://arxiv.org/abs/2006.16668

### 40 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)


### 41 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)
Paper: https://arxiv.org/abs/2006.15222

### 42 48:06  Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)
Paper: https://arxiv.org/abs/2006.16236

### 43 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality


### 44 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality
Paper: https://arxiv.org/abs/1310.4546

### 45 34:30  Big Bird: Transformers for Longer Sequences (Paper Explained)
Paper: https://arxiv.org/abs/2007.14062

### 46 1:05:16  Hopfield Networks is All You Need (Paper Explained)
Paper: https://arxiv.org/abs/2008.02217

### 47 1:00:41  REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)
Paper: https://arxiv.org/abs/2002.08909

### 48 45:30  Learning to summarize from human feedback (Paper Explained)
Paper: https://arxiv.org/abs/2009.01325

### 49 54:39  Rethinking Attention with Performers (Paper Explained)
Paper: https://arxiv.org/abs/2009.14794

### 50 52:16  Language Models are Open Knowledge Graphs (Paper Explained)
Paper: https://arxiv.org/abs/2010.11967

### 51 1:03:18  Extracting Training Data from Large Language Models (Paper Explained)
Paper: https://arxiv.org/abs/2012.07805

### 52 55:46  OpenAI DALL¬∑E: Creating Images from Text (Blog Post Explained)
Blog: https://openai.com/blog/dall-e/

### 53 48:07  OpenAI CLIP: ConnectingText and Images (Paper Explained)
Paper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf

### 54 33:47  Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Paper: https://arxiv.org/abs/2101.03961

### 55 43:51  Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)
Paper: https://arxiv.org/abs/2002.09402

### 56 48:12  Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)
Paper: https://arxiv.org/abs/2102.03902

### 57 45:14  DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)
Paper: https://arxiv.org/abs/2006.03654

### 58 31:22  ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation
Paper: https://ofir.io/train_short_test_long.pdf

### 59 36:37  ‚àû-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)
Paper: https://arxiv.org/abs/2109.00301

### 60 13:19  Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset
Paper: https://arxiv.org/abs/2109.07958

### 61 45:22  Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)
Paper: https://arxiv.org/abs/2110.07178

### 62 57:07  Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained
Paper: https://arxiv.org/abs/2111.12763

-------
-------


# Abhishek Thakur

-------

# Approaching (Almost) Any NLP Problem
https://www.youtube.com/playlist?list=PL98nY_tJQXZk-NeS9jqeH2iY4-IvoYbRC



### 1 13:17  What is stemming and lemmatization?


### 2 20:22  Natural Language Processing: Tokenization (Basic)


### 3 19:30  Subword Tokenization: Byte Pair Encoding


-------


# BERT
https://www.youtube.com/playlist?list=PL98nY_tJQXZl0WwsJluhc6tGrKWCX2suH


### 1 1:01:15  BERT on Steroids: Fine-tuning BERT for a dataset using PyTorch and Google Cloud TPUs


### 2 34:20 Training BERT Language Model From Scratch On TPUs


### 3 1:16:06 Training Sentiment Model Using BERT and Serving it with Flask API


-------

# Approaching (Almost) Any Machine Learning Problem
https://github.com/abhishekkrthakur/approachingalmost

AAAMLP.pdf: https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf

AAAMLP.pdf: https://github.com/grapestone5321/Kaggle-Natural-Language-Processing/blob/main/docs/AAAMLP.pdf

### Approaching text classification/regression

-------
-------


