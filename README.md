# Kaggle-Natural-Language-Processing
Kaggle-Natural-Language-Processing



-------
-------

# Natural Language Processing
https://www.youtube.com/playlist?list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu

Yannic Kilcher

### 1 27:33  GPT-2: Language Models are Unsupervised Multitask Learners


### 2 40:13  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding


### 3 18:19  Stochastic RNNs without Teacher-Forcing


### 4 27:07  Attention Is All You Need


### 5 30:06  XLNet: Generalized Autoregressive Pretraining for Language Understanding


### 6 19:15  RoBERTa: A Robustly Optimized BERT Pretraining Approach


### 7 30:22  LeDeepChef üë®‚Äçüç≥ Deep Reinforcement Learning Agent for Families of Text-Based Games


### 8 29:12  Reformer: The Efficient Transformer


### 9 21:18  Turing-NLG, DeepSpeed and the ZeRO optimizer


### 10 24:16  Deep Learning for Symbolic Mathematics


### 11 18:59  Evaluating NLP Models via Contrast Sets


### 12 18:15  Imputer: Sequence Modelling via Imputation and Dynamic Programming


### 13 26:36  Longformer: The Long-Document Transformer


### 14 11:21  I talk to the new Facebook Blender Chatbot


### 15 31:48  TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)


### 16 1:02:41  [Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)


### 17 11:11  [News] OpenAI Model Generates Python Code


### 18 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)


### 19 53:35  When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)


### 20 1:04:30  GPT-3: Language Models are Few-Shot Learners (Paper Explained)


### 21 48:21  Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)


### 22 30:11  Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)


### 23 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)


### 24 31:35  BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)


### 25 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)


### 26 48:38  TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)


### 27 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 28 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 29 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 30 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 31 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 32 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 33 50:24  Linformer: Self-Attention with Linear Complexity (Paper Explained)


### 34 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)


### 35 29:42  VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)


### 36 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)


### 37 36:49  Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)


### 38 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)


### 39 1:13:04 GShard:  Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)


### 40 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)


### 41 36:50  BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)


### 42 48:06  Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)


### 43 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality


### 44 31:22  [Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality


### 45 34:30  Big Bird: Transformers for Longer Sequences (Paper Explained)


### 46 1:05:16  Hopfield Networks is All You Need (Paper Explained)


### 47 1:00:41  REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)


### 48 45:30  Learning to summarize from human feedback (Paper Explained)


### 49 54:39  Rethinking Attention with Performers (Paper Explained)


### 50 52:16  Language Models are Open Knowledge Graphs (Paper Explained)


### 51 1:03:18  Extracting Training Data from Large Language Models (Paper Explained)


### 52 55:46  OpenAI DALL¬∑E: Creating Images from Text (Blog Post Explained)


### 53 48:07  OpenAI CLIP: ConnectingText and Images (Paper Explained)


### 54 33:47  Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity


### 55 43:51  Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)


### 56 48:12  Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)


### 57 45:14  DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)


### 58 31:22  ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation


### 59 36:37  ‚àû-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)


### 60 13:19  Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset


### 61 45:22  Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)


### 62 57:07  Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained


-------
-------

